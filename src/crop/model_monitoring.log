2021-12-14 19:58:54,669 root         INFO     Namespace(gpu=True, train=True, batch_size=8, num_workers=5, num_epochs=20, num_output_classes=32, learning_rate=0.01, sgd_momentum=0.5, plot_output_path='./Plots_', model_path=None, epoch_save_checkpoint=5, split_percentage=0.01, patience=50, transforms='crop', pred_model='./checkpoint_model.pth')
2021-12-14 19:58:54,670 root         INFO     color transforms added - 
2021-12-14 19:58:54,700 root         INFO     Settings for Cuda
2021-12-14 19:58:54,701 root         INFO     Training Segnet
2021-12-14 19:58:54,701 root         INFO     Generating DataLoader
2021-12-14 19:58:54,702 root         INFO     Training 1st Model
2021-12-14 19:58:57,672 root         INFO     SegNet(
  (torch_vgg16): VGG(
    (features): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): ReLU(inplace=True)
      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (3): ReLU(inplace=True)
      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (6): ReLU(inplace=True)
      (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (8): ReLU(inplace=True)
      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (11): ReLU(inplace=True)
      (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (13): ReLU(inplace=True)
      (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (15): ReLU(inplace=True)
      (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (18): ReLU(inplace=True)
      (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (20): ReLU(inplace=True)
      (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (22): ReLU(inplace=True)
      (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (25): ReLU(inplace=True)
      (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (27): ReLU(inplace=True)
      (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (29): ReLU(inplace=True)
      (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))
    (classifier): Sequential(
      (0): Linear(in_features=25088, out_features=4096, bias=True)
      (1): ReLU(inplace=True)
      (2): Dropout(p=0.5, inplace=False)
      (3): Linear(in_features=4096, out_features=4096, bias=True)
      (4): ReLU(inplace=True)
      (5): Dropout(p=0.5, inplace=False)
      (6): Linear(in_features=4096, out_features=1000, bias=True)
    )
  )
  (encoder_stage_1_conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (encoder_stage_1_conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (encoder_stage_2_conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (encoder_stage_2_conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (encoder_stage_3_conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (encoder_stage_3_conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (encoder_stage_3_conv3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (encoder_stage_4_conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (encoder_stage_4_conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (encoder_stage_4_conv3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (encoder_stage_5_conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (encoder_stage_5_conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (encoder_stage_5_conv3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (encoder_stage_1_batch_normalization1): BatchNorm2d(64, eps=1e-05, momentum=0.5, affine=True, track_running_stats=True)
  (encoder_stage_2_batch_normalization1): BatchNorm2d(128, eps=1e-05, momentum=0.5, affine=True, track_running_stats=True)
  (encoder_stage_3_batch_normalization1): BatchNorm2d(256, eps=1e-05, momentum=0.5, affine=True, track_running_stats=True)
  (encoder_stage_4_batch_normalization1): BatchNorm2d(512, eps=1e-05, momentum=0.5, affine=True, track_running_stats=True)
  (encoder_stage_5_batch_normalization1): BatchNorm2d(512, eps=1e-05, momentum=0.5, affine=True, track_running_stats=True)
  (encoder_stage_1_batch_normalization2): BatchNorm2d(64, eps=1e-05, momentum=0.5, affine=True, track_running_stats=True)
  (encoder_stage_2_batch_normalization2): BatchNorm2d(128, eps=1e-05, momentum=0.5, affine=True, track_running_stats=True)
  (encoder_stage_3_batch_normalization2): BatchNorm2d(256, eps=1e-05, momentum=0.5, affine=True, track_running_stats=True)
  (encoder_stage_4_batch_normalization2): BatchNorm2d(512, eps=1e-05, momentum=0.5, affine=True, track_running_stats=True)
  (encoder_stage_5_batch_normalization2): BatchNorm2d(512, eps=1e-05, momentum=0.5, affine=True, track_running_stats=True)
  (encoder_stage_3_batch_normalization3): BatchNorm2d(256, eps=1e-05, momentum=0.5, affine=True, track_running_stats=True)
  (encoder_stage_4_batch_normalization3): BatchNorm2d(512, eps=1e-05, momentum=0.5, affine=True, track_running_stats=True)
  (encoder_stage_5_batch_normalization3): BatchNorm2d(512, eps=1e-05, momentum=0.5, affine=True, track_running_stats=True)
  (relu): ReLU()
  (encoder_max_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (decoder_unpool): MaxUnpool2d(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))
  (decoder_stage_1_conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (decoder_stage_1_conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (decoder_stage_1_conv3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (decoder_stage_2_conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (decoder_stage_2_conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (decoder_stage_2_conv3): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (decoder_stage_3_conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (decoder_stage_3_conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (decoder_stage_3_conv3): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (decoder_stage_4_conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (decoder_stage_4_conv2): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (decoder_stage_5_conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (decoder_stage_5_conv2): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (decoder_stage_1_batch_normalization1): BatchNorm2d(512, eps=1e-05, momentum=0.5, affine=True, track_running_stats=True)
  (decoder_stage_2_batch_normalization1): BatchNorm2d(512, eps=1e-05, momentum=0.5, affine=True, track_running_stats=True)
  (decoder_stage_3_batch_normalization1): BatchNorm2d(256, eps=1e-05, momentum=0.5, affine=True, track_running_stats=True)
  (decoder_stage_4_batch_normalization1): BatchNorm2d(128, eps=1e-05, momentum=0.5, affine=True, track_running_stats=True)
  (decoder_stage_5_batch_normalization1): BatchNorm2d(64, eps=1e-05, momentum=0.5, affine=True, track_running_stats=True)
  (decoder_stage_1_batch_normalization2): BatchNorm2d(512, eps=1e-05, momentum=0.5, affine=True, track_running_stats=True)
  (decoder_stage_2_batch_normalization2): BatchNorm2d(512, eps=1e-05, momentum=0.5, affine=True, track_running_stats=True)
  (decoder_stage_3_batch_normalization2): BatchNorm2d(256, eps=1e-05, momentum=0.5, affine=True, track_running_stats=True)
  (decoder_stage_4_batch_normalization2): BatchNorm2d(64, eps=1e-05, momentum=0.5, affine=True, track_running_stats=True)
  (decoder_stage_1_batch_normalization3): BatchNorm2d(512, eps=1e-05, momentum=0.5, affine=True, track_running_stats=True)
  (decoder_stage_2_batch_normalization3): BatchNorm2d(256, eps=1e-05, momentum=0.5, affine=True, track_running_stats=True)
  (decoder_stage_3_batch_normalization3): BatchNorm2d(128, eps=1e-05, momentum=0.5, affine=True, track_running_stats=True)
)
2021-12-14 19:58:57,673 root         INFO     Epoch 1
2021-12-14 19:59:01,334 root         INFO     Epoch: 1 	 iteration: 0 	 Training Loss Current:3.5222 Average:(3.5222)
2021-12-14 19:59:47,058 root         INFO     Epoch: 1 	 iteration: 50 	 Training Loss Current:1.6864 Average:(2.1102)
2021-12-14 20:00:19,458 root         INFO     Average Loss for epoch 1 is 1.8745459912489746
2021-12-14 20:00:21,162 root         INFO     Validation Loss Current:1.4076 Average:(1.4076)
2021-12-14 20:00:21,221 root         INFO     Current Epoch loss is better : True 	 Old Loss: inf  vs New loss:1.40762460231781
2021-12-14 20:00:21,221 root         INFO     New Best Identified: 	 Old Loss: inf  vs New loss:	1.40762460231781 
2021-12-14 20:00:30,812 root         INFO     Epoch 2
2021-12-14 20:00:34,020 root         INFO     Epoch: 2 	 iteration: 0 	 Training Loss Current:1.5715 Average:(1.5715)
2021-12-14 20:01:19,702 root         INFO     Epoch: 2 	 iteration: 50 	 Training Loss Current:1.3425 Average:(1.3638)
2021-12-14 20:01:52,203 root         INFO     Average Loss for epoch 2 is 1.302843340879215
2021-12-14 20:01:53,947 root         INFO     Validation Loss Current:1.1567 Average:(1.1567)
2021-12-14 20:01:54,022 root         INFO     Current Epoch loss is better : True 	 Old Loss: 1.40762460231781  vs New loss:1.1567087173461914
2021-12-14 20:01:54,022 root         INFO     New Best Identified: 	 Old Loss: 1.40762460231781  vs New loss:	1.1567087173461914 
2021-12-14 20:02:02,931 root         INFO     Epoch 3
2021-12-14 20:02:06,124 root         INFO     Epoch: 3 	 iteration: 0 	 Training Loss Current:1.2664 Average:(1.2664)
2021-12-14 20:02:51,891 root         INFO     Epoch: 3 	 iteration: 50 	 Training Loss Current:1.1001 Average:(1.1259)
2021-12-14 20:03:24,400 root         INFO     Average Loss for epoch 3 is 1.0865445102669664
2021-12-14 20:03:26,123 root         INFO     Validation Loss Current:1.0843 Average:(1.0843)
2021-12-14 20:03:26,194 root         INFO     Current Epoch loss is better : True 	 Old Loss: 1.1567087173461914  vs New loss:1.0843080282211304
2021-12-14 20:03:26,194 root         INFO     New Best Identified: 	 Old Loss: 1.1567087173461914  vs New loss:	1.0843080282211304 
2021-12-14 20:03:35,431 root         INFO     Epoch 4
2021-12-14 20:03:38,582 root         INFO     Epoch: 4 	 iteration: 0 	 Training Loss Current:0.9696 Average:(0.9696)
2021-12-14 20:04:24,285 root         INFO     Epoch: 4 	 iteration: 50 	 Training Loss Current:1.0121 Average:(0.9768)
2021-12-14 20:04:56,811 root         INFO     Average Loss for epoch 4 is 0.9756995350208337
2021-12-14 20:04:58,529 root         INFO     Validation Loss Current:0.8793 Average:(0.8793)
2021-12-14 20:04:58,600 root         INFO     Current Epoch loss is better : True 	 Old Loss: 1.0843080282211304  vs New loss:0.8792766332626343
2021-12-14 20:04:58,600 root         INFO     New Best Identified: 	 Old Loss: 1.0843080282211304  vs New loss:	0.8792766332626343 
2021-12-14 20:05:08,661 root         INFO     Epoch 5
2021-12-14 20:05:11,883 root         INFO     Epoch: 5 	 iteration: 0 	 Training Loss Current:0.8703 Average:(0.8703)
2021-12-14 20:05:57,640 root         INFO     Epoch: 5 	 iteration: 50 	 Training Loss Current:0.9112 Average:(0.8955)
2021-12-14 20:06:30,165 root         INFO     Average Loss for epoch 5 is 0.8990754412986359
2021-12-14 20:06:31,878 root         INFO     Validation Loss Current:0.8141 Average:(0.8141)
2021-12-14 20:06:31,942 root         INFO     Current Epoch loss is better : True 	 Old Loss: 0.8792766332626343  vs New loss:0.8140726685523987
2021-12-14 20:06:31,942 root         INFO     Saving Checkpoint for SegNet at epoch 5
2021-12-14 20:06:52,765 root         INFO     checkpoint saved at SegNet.pth_5.tar
2021-12-14 20:06:52,766 root         INFO     New Best Identified: 	 Old Loss: 0.8792766332626343  vs New loss:	0.8140726685523987 
2021-12-14 20:07:02,491 root         INFO     Epoch 6
2021-12-14 20:07:05,825 root         INFO     Epoch: 6 	 iteration: 0 	 Training Loss Current:0.7380 Average:(0.7380)
2021-12-14 20:07:51,433 root         INFO     Epoch: 6 	 iteration: 50 	 Training Loss Current:0.7690 Average:(0.8523)
2021-12-14 20:08:23,934 root         INFO     Average Loss for epoch 6 is 0.834238357605783
2021-12-14 20:08:25,658 root         INFO     Validation Loss Current:0.7687 Average:(0.7687)
2021-12-14 20:08:25,726 root         INFO     Current Epoch loss is better : True 	 Old Loss: 0.8140726685523987  vs New loss:0.7687059640884399
2021-12-14 20:08:25,726 root         INFO     New Best Identified: 	 Old Loss: 0.8140726685523987  vs New loss:	0.7687059640884399 
2021-12-14 20:08:35,664 root         INFO     Epoch 7
2021-12-14 20:08:38,830 root         INFO     Epoch: 7 	 iteration: 0 	 Training Loss Current:0.7210 Average:(0.7210)
2021-12-14 20:09:24,629 root         INFO     Epoch: 7 	 iteration: 50 	 Training Loss Current:0.8163 Average:(0.7934)
2021-12-14 20:09:57,193 root         INFO     Average Loss for epoch 7 is 0.7837579246900267
2021-12-14 20:09:58,911 root         INFO     Validation Loss Current:0.7758 Average:(0.7758)
2021-12-14 20:09:58,998 root         INFO     Current Epoch loss is better : False 	 Old Loss: 0.7687059640884399  vs New loss:0.7757601141929626
2021-12-14 20:09:58,998 root         INFO     Loss didnot improve
2021-12-14 20:09:58,998 root         INFO     Epoch 8
2021-12-14 20:10:02,159 root         INFO     Epoch: 8 	 iteration: 0 	 Training Loss Current:0.7267 Average:(0.7267)
2021-12-14 20:10:48,073 root         INFO     Epoch: 8 	 iteration: 50 	 Training Loss Current:0.8296 Average:(0.7445)
2021-12-14 20:11:20,651 root         INFO     Average Loss for epoch 8 is 0.7374099901842452
2021-12-14 20:11:22,379 root         INFO     Validation Loss Current:0.7008 Average:(0.7008)
2021-12-14 20:11:22,447 root         INFO     Current Epoch loss is better : True 	 Old Loss: 0.7687059640884399  vs New loss:0.7008230686187744
2021-12-14 20:11:22,447 root         INFO     New Best Identified: 	 Old Loss: 0.7687059640884399  vs New loss:	0.7008230686187744 
2021-12-14 20:11:32,314 root         INFO     Epoch 9
2021-12-14 20:11:35,588 root         INFO     Epoch: 9 	 iteration: 0 	 Training Loss Current:0.6709 Average:(0.6709)
2021-12-14 20:12:21,355 root         INFO     Epoch: 9 	 iteration: 50 	 Training Loss Current:0.6830 Average:(0.7074)
2021-12-14 20:12:53,930 root         INFO     Average Loss for epoch 9 is 0.7081393704290693
2021-12-14 20:12:55,664 root         INFO     Validation Loss Current:0.6937 Average:(0.6937)
2021-12-14 20:12:55,734 root         INFO     Current Epoch loss is better : True 	 Old Loss: 0.7008230686187744  vs New loss:0.6937340497970581
2021-12-14 20:12:55,735 root         INFO     New Best Identified: 	 Old Loss: 0.7008230686187744  vs New loss:	0.6937340497970581 
2021-12-14 20:13:06,175 root         INFO     Epoch 10
2021-12-14 20:13:09,362 root         INFO     Epoch: 10 	 iteration: 0 	 Training Loss Current:0.7032 Average:(0.7032)
2021-12-14 20:13:55,075 root         INFO     Epoch: 10 	 iteration: 50 	 Training Loss Current:0.6556 Average:(0.6574)
2021-12-14 20:14:27,633 root         INFO     Average Loss for epoch 10 is 0.6719541924144075
2021-12-14 20:14:29,352 root         INFO     Validation Loss Current:0.6687 Average:(0.6687)
2021-12-14 20:14:29,423 root         INFO     Current Epoch loss is better : True 	 Old Loss: 0.6937340497970581  vs New loss:0.6687115430831909
2021-12-14 20:14:29,423 root         INFO     Saving Checkpoint for SegNet at epoch 10
2021-12-14 20:14:49,371 root         INFO     checkpoint saved at SegNet.pth_10.tar
2021-12-14 20:14:49,372 root         INFO     New Best Identified: 	 Old Loss: 0.6937340497970581  vs New loss:	0.6687115430831909 
2021-12-14 20:15:00,006 root         INFO     Epoch 11
2021-12-14 20:15:03,184 root         INFO     Epoch: 11 	 iteration: 0 	 Training Loss Current:0.6383 Average:(0.6383)
2021-12-14 20:15:48,878 root         INFO     Epoch: 11 	 iteration: 50 	 Training Loss Current:0.5521 Average:(0.6365)
2021-12-14 20:16:21,415 root         INFO     Average Loss for epoch 11 is 0.6366160295538669
2021-12-14 20:16:23,142 root         INFO     Validation Loss Current:0.6720 Average:(0.6720)
2021-12-14 20:16:23,210 root         INFO     Current Epoch loss is better : False 	 Old Loss: 0.6687115430831909  vs New loss:0.6720266938209534
2021-12-14 20:16:23,211 root         INFO     Loss didnot improve
2021-12-14 20:16:23,211 root         INFO     Epoch 12
2021-12-14 20:16:26,380 root         INFO     Epoch: 12 	 iteration: 0 	 Training Loss Current:0.6167 Average:(0.6167)
2021-12-14 20:17:12,241 root         INFO     Epoch: 12 	 iteration: 50 	 Training Loss Current:0.5082 Average:(0.6020)
2021-12-14 20:17:44,834 root         INFO     Average Loss for epoch 12 is 0.6056258405319895
2021-12-14 20:17:46,564 root         INFO     Validation Loss Current:0.5785 Average:(0.5785)
2021-12-14 20:17:46,641 root         INFO     Current Epoch loss is better : True 	 Old Loss: 0.6687115430831909  vs New loss:0.5785004496574402
2021-12-14 20:17:46,641 root         INFO     New Best Identified: 	 Old Loss: 0.6687115430831909  vs New loss:	0.5785004496574402 
2021-12-14 20:17:55,464 root         INFO     Epoch 13
2021-12-14 20:17:58,730 root         INFO     Epoch: 13 	 iteration: 0 	 Training Loss Current:0.5679 Average:(0.5679)
2021-12-14 20:18:44,440 root         INFO     Epoch: 13 	 iteration: 50 	 Training Loss Current:0.5431 Average:(0.5869)
2021-12-14 20:19:17,016 root         INFO     Average Loss for epoch 13 is 0.5887429932352445
2021-12-14 20:19:18,769 root         INFO     Validation Loss Current:0.5632 Average:(0.5632)
2021-12-14 20:19:18,859 root         INFO     Current Epoch loss is better : True 	 Old Loss: 0.5785004496574402  vs New loss:0.5631524324417114
2021-12-14 20:19:18,859 root         INFO     New Best Identified: 	 Old Loss: 0.5785004496574402  vs New loss:	0.5631524324417114 
2021-12-14 20:19:29,682 root         INFO     Epoch 14
2021-12-14 20:19:32,970 root         INFO     Epoch: 14 	 iteration: 0 	 Training Loss Current:0.5472 Average:(0.5472)
2021-12-14 20:20:18,700 root         INFO     Epoch: 14 	 iteration: 50 	 Training Loss Current:0.5189 Average:(0.5550)
2021-12-14 20:20:51,300 root         INFO     Average Loss for epoch 14 is 0.5641244385359266
2021-12-14 20:20:53,038 root         INFO     Validation Loss Current:0.5840 Average:(0.5840)
2021-12-14 20:20:53,118 root         INFO     Current Epoch loss is better : False 	 Old Loss: 0.5631524324417114  vs New loss:0.5839796662330627
2021-12-14 20:20:53,118 root         INFO     Loss didnot improve
2021-12-14 20:20:53,118 root         INFO     Epoch 15
2021-12-14 20:20:56,316 root         INFO     Epoch: 15 	 iteration: 0 	 Training Loss Current:0.5658 Average:(0.5658)
2021-12-14 20:21:42,229 root         INFO     Epoch: 15 	 iteration: 50 	 Training Loss Current:0.4556 Average:(0.5567)
2021-12-14 20:22:14,816 root         INFO     Average Loss for epoch 15 is 0.5511575021730033
2021-12-14 20:22:16,563 root         INFO     Validation Loss Current:0.5266 Average:(0.5266)
2021-12-14 20:22:16,635 root         INFO     Current Epoch loss is better : True 	 Old Loss: 0.5631524324417114  vs New loss:0.5265756249427795
2021-12-14 20:22:16,635 root         INFO     Saving Checkpoint for SegNet at epoch 15
2021-12-14 20:22:38,682 root         INFO     checkpoint saved at SegNet.pth_15.tar
2021-12-14 20:22:38,682 root         INFO     New Best Identified: 	 Old Loss: 0.5631524324417114  vs New loss:	0.5265756249427795 
2021-12-14 20:22:48,993 root         INFO     Epoch 16
2021-12-14 20:22:52,293 root         INFO     Epoch: 16 	 iteration: 0 	 Training Loss Current:0.6061 Average:(0.6061)
2021-12-14 20:23:37,970 root         INFO     Epoch: 16 	 iteration: 50 	 Training Loss Current:0.4968 Average:(0.5347)
2021-12-14 20:24:10,506 root         INFO     Average Loss for epoch 16 is 0.5253580986422833
2021-12-14 20:24:12,236 root         INFO     Validation Loss Current:0.5577 Average:(0.5577)
2021-12-14 20:24:12,312 root         INFO     Current Epoch loss is better : False 	 Old Loss: 0.5265756249427795  vs New loss:0.5576704144477844
2021-12-14 20:24:12,312 root         INFO     Loss didnot improve
2021-12-14 20:24:12,312 root         INFO     Epoch 17
2021-12-14 20:24:15,453 root         INFO     Epoch: 17 	 iteration: 0 	 Training Loss Current:0.4568 Average:(0.4568)
2021-12-14 20:25:01,388 root         INFO     Epoch: 17 	 iteration: 50 	 Training Loss Current:0.4078 Average:(0.5093)
2021-12-14 20:25:34,048 root         INFO     Average Loss for epoch 17 is 0.5094018521844825
2021-12-14 20:25:35,794 root         INFO     Validation Loss Current:0.5399 Average:(0.5399)
2021-12-14 20:25:35,876 root         INFO     Current Epoch loss is better : False 	 Old Loss: 0.5265756249427795  vs New loss:0.5398566126823425
2021-12-14 20:25:35,876 root         INFO     Loss didnot improve
2021-12-14 20:25:35,876 root         INFO     Epoch 18
2021-12-14 20:25:39,099 root         INFO     Epoch: 18 	 iteration: 0 	 Training Loss Current:0.5594 Average:(0.5594)
2021-12-14 20:26:24,966 root         INFO     Epoch: 18 	 iteration: 50 	 Training Loss Current:0.5240 Average:(0.4896)
2021-12-14 20:26:57,573 root         INFO     Average Loss for epoch 18 is 0.5014045465233004
2021-12-14 20:26:59,315 root         INFO     Validation Loss Current:0.5381 Average:(0.5381)
2021-12-14 20:26:59,398 root         INFO     Current Epoch loss is better : False 	 Old Loss: 0.5265756249427795  vs New loss:0.538123607635498
2021-12-14 20:26:59,398 root         INFO     Loss didnot improve
2021-12-14 20:26:59,398 root         INFO     Epoch 19
2021-12-14 20:27:02,601 root         INFO     Epoch: 19 	 iteration: 0 	 Training Loss Current:0.4961 Average:(0.4961)
2021-12-14 20:27:48,557 root         INFO     Epoch: 19 	 iteration: 50 	 Training Loss Current:0.5403 Average:(0.4971)
2021-12-14 20:28:21,216 root         INFO     Average Loss for epoch 19 is 0.4854956738199899
2021-12-14 20:28:22,963 root         INFO     Validation Loss Current:0.4909 Average:(0.4909)
2021-12-14 20:28:23,032 root         INFO     Current Epoch loss is better : True 	 Old Loss: 0.5265756249427795  vs New loss:0.49090224504470825
2021-12-14 20:28:23,033 root         INFO     New Best Identified: 	 Old Loss: 0.5265756249427795  vs New loss:	0.49090224504470825 
2021-12-14 20:28:33,277 root         INFO     Epoch 20
2021-12-14 20:28:36,565 root         INFO     Epoch: 20 	 iteration: 0 	 Training Loss Current:0.4817 Average:(0.4817)
2021-12-14 20:29:22,382 root         INFO     Epoch: 20 	 iteration: 50 	 Training Loss Current:0.4646 Average:(0.4729)
2021-12-14 20:29:54,947 root         INFO     Average Loss for epoch 20 is 0.47500908048420887
2021-12-14 20:29:56,678 root         INFO     Validation Loss Current:0.5201 Average:(0.5201)
2021-12-14 20:29:56,755 root         INFO     Current Epoch loss is better : False 	 Old Loss: 0.49090224504470825  vs New loss:0.5200945138931274
2021-12-14 20:29:56,755 root         INFO     Saving Checkpoint for SegNet at epoch 20
2021-12-14 20:30:17,140 root         INFO     checkpoint saved at SegNet.pth_20.tar
2021-12-14 20:30:17,141 root         INFO     Loss didnot improve
2021-12-14 20:30:37,173 root         INFO     checkpoint saved at SegNet.pth.tar
2021-12-14 20:30:37,624 root         INFO     Plotting Charts
2021-12-14 20:30:37,624 root         INFO     Train Losses:[1.8745459912489746, 1.302843340879215, 1.0865445102669664, 0.9756995350208337, 0.8990754412986359, 0.834238357605783, 0.7837579246900267, 0.7374099901842452, 0.7081393704290693, 0.6719541924144075, 0.6366160295538669, 0.6056258405319895, 0.5887429932352445, 0.5641244385359266, 0.5511575021730033, 0.5253580986422833, 0.5094018521844825, 0.5014045465233004, 0.4854956738199899, 0.47500908048420887]
2021-12-14 20:30:37,624 root         INFO     Validation Losses:[1.40762460231781, 1.1567087173461914, 1.0843080282211304, 0.8792766332626343, 0.8140726685523987, 0.7687059640884399, 0.7757601141929626, 0.7008230686187744, 0.6937340497970581, 0.6687115430831909, 0.6720266938209534, 0.5785004496574402, 0.5631524324417114, 0.5839796662330627, 0.5265756249427795, 0.5576704144477844, 0.5398566126823425, 0.538123607635498, 0.49090224504470825, 0.5200945138931274]
2021-12-14 20:30:37,784 root         INFO     loss curve saved at ./Plots_loss_curves.jpg.
2021-12-14 20:30:37,784 root         INFO     Training Finished for SegNet
