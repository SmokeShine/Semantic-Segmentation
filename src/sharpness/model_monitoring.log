2021-12-14 18:07:12,919 root         INFO     Namespace(gpu=True, train=True, batch_size=8, num_workers=5, num_epochs=20, num_output_classes=32, learning_rate=0.01, sgd_momentum=0.5, plot_output_path='./Plots_', model_path=None, epoch_save_checkpoint=5, split_percentage=0.01, patience=50, transforms='sharpness', pred_model='./checkpoint_model.pth')
2021-12-14 18:07:12,919 root         INFO     color transforms added - sharpness,
2021-12-14 18:07:12,949 root         INFO     Settings for Cuda
2021-12-14 18:07:12,949 root         INFO     Training Segnet
2021-12-14 18:07:12,949 root         INFO     Generating DataLoader
2021-12-14 18:07:12,950 root         INFO     Training 1st Model
2021-12-14 18:07:15,850 root         INFO     SegNet(
  (torch_vgg16): VGG(
    (features): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): ReLU(inplace=True)
      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (3): ReLU(inplace=True)
      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (6): ReLU(inplace=True)
      (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (8): ReLU(inplace=True)
      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (11): ReLU(inplace=True)
      (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (13): ReLU(inplace=True)
      (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (15): ReLU(inplace=True)
      (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (18): ReLU(inplace=True)
      (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (20): ReLU(inplace=True)
      (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (22): ReLU(inplace=True)
      (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (25): ReLU(inplace=True)
      (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (27): ReLU(inplace=True)
      (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (29): ReLU(inplace=True)
      (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))
    (classifier): Sequential(
      (0): Linear(in_features=25088, out_features=4096, bias=True)
      (1): ReLU(inplace=True)
      (2): Dropout(p=0.5, inplace=False)
      (3): Linear(in_features=4096, out_features=4096, bias=True)
      (4): ReLU(inplace=True)
      (5): Dropout(p=0.5, inplace=False)
      (6): Linear(in_features=4096, out_features=1000, bias=True)
    )
  )
  (encoder_stage_1_conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (encoder_stage_1_conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (encoder_stage_2_conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (encoder_stage_2_conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (encoder_stage_3_conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (encoder_stage_3_conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (encoder_stage_3_conv3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (encoder_stage_4_conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (encoder_stage_4_conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (encoder_stage_4_conv3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (encoder_stage_5_conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (encoder_stage_5_conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (encoder_stage_5_conv3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (encoder_stage_1_batch_normalization1): BatchNorm2d(64, eps=1e-05, momentum=0.5, affine=True, track_running_stats=True)
  (encoder_stage_2_batch_normalization1): BatchNorm2d(128, eps=1e-05, momentum=0.5, affine=True, track_running_stats=True)
  (encoder_stage_3_batch_normalization1): BatchNorm2d(256, eps=1e-05, momentum=0.5, affine=True, track_running_stats=True)
  (encoder_stage_4_batch_normalization1): BatchNorm2d(512, eps=1e-05, momentum=0.5, affine=True, track_running_stats=True)
  (encoder_stage_5_batch_normalization1): BatchNorm2d(512, eps=1e-05, momentum=0.5, affine=True, track_running_stats=True)
  (encoder_stage_1_batch_normalization2): BatchNorm2d(64, eps=1e-05, momentum=0.5, affine=True, track_running_stats=True)
  (encoder_stage_2_batch_normalization2): BatchNorm2d(128, eps=1e-05, momentum=0.5, affine=True, track_running_stats=True)
  (encoder_stage_3_batch_normalization2): BatchNorm2d(256, eps=1e-05, momentum=0.5, affine=True, track_running_stats=True)
  (encoder_stage_4_batch_normalization2): BatchNorm2d(512, eps=1e-05, momentum=0.5, affine=True, track_running_stats=True)
  (encoder_stage_5_batch_normalization2): BatchNorm2d(512, eps=1e-05, momentum=0.5, affine=True, track_running_stats=True)
  (encoder_stage_3_batch_normalization3): BatchNorm2d(256, eps=1e-05, momentum=0.5, affine=True, track_running_stats=True)
  (encoder_stage_4_batch_normalization3): BatchNorm2d(512, eps=1e-05, momentum=0.5, affine=True, track_running_stats=True)
  (encoder_stage_5_batch_normalization3): BatchNorm2d(512, eps=1e-05, momentum=0.5, affine=True, track_running_stats=True)
  (relu): ReLU()
  (encoder_max_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (decoder_unpool): MaxUnpool2d(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))
  (decoder_stage_1_conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (decoder_stage_1_conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (decoder_stage_1_conv3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (decoder_stage_2_conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (decoder_stage_2_conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (decoder_stage_2_conv3): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (decoder_stage_3_conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (decoder_stage_3_conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (decoder_stage_3_conv3): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (decoder_stage_4_conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (decoder_stage_4_conv2): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (decoder_stage_5_conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (decoder_stage_5_conv2): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (decoder_stage_1_batch_normalization1): BatchNorm2d(512, eps=1e-05, momentum=0.5, affine=True, track_running_stats=True)
  (decoder_stage_2_batch_normalization1): BatchNorm2d(512, eps=1e-05, momentum=0.5, affine=True, track_running_stats=True)
  (decoder_stage_3_batch_normalization1): BatchNorm2d(256, eps=1e-05, momentum=0.5, affine=True, track_running_stats=True)
  (decoder_stage_4_batch_normalization1): BatchNorm2d(128, eps=1e-05, momentum=0.5, affine=True, track_running_stats=True)
  (decoder_stage_5_batch_normalization1): BatchNorm2d(64, eps=1e-05, momentum=0.5, affine=True, track_running_stats=True)
  (decoder_stage_1_batch_normalization2): BatchNorm2d(512, eps=1e-05, momentum=0.5, affine=True, track_running_stats=True)
  (decoder_stage_2_batch_normalization2): BatchNorm2d(512, eps=1e-05, momentum=0.5, affine=True, track_running_stats=True)
  (decoder_stage_3_batch_normalization2): BatchNorm2d(256, eps=1e-05, momentum=0.5, affine=True, track_running_stats=True)
  (decoder_stage_4_batch_normalization2): BatchNorm2d(64, eps=1e-05, momentum=0.5, affine=True, track_running_stats=True)
  (decoder_stage_1_batch_normalization3): BatchNorm2d(512, eps=1e-05, momentum=0.5, affine=True, track_running_stats=True)
  (decoder_stage_2_batch_normalization3): BatchNorm2d(256, eps=1e-05, momentum=0.5, affine=True, track_running_stats=True)
  (decoder_stage_3_batch_normalization3): BatchNorm2d(128, eps=1e-05, momentum=0.5, affine=True, track_running_stats=True)
)
2021-12-14 18:07:15,851 root         INFO     Epoch 1
2021-12-14 18:07:19,473 root         INFO     Epoch: 1 	 iteration: 0 	 Training Loss Current:3.5219 Average:(3.5219)
2021-12-14 18:08:06,019 root         INFO     Epoch: 1 	 iteration: 50 	 Training Loss Current:1.6800 Average:(2.1085)
2021-12-14 18:08:39,439 root         INFO     Average Loss for epoch 1 is 1.8731278902515553
2021-12-14 18:08:41,224 root         INFO     Validation Loss Current:1.3758 Average:(1.3758)
2021-12-14 18:08:41,298 root         INFO     Current Epoch loss is better : True 	 Old Loss: inf  vs New loss:1.3757574558258057
2021-12-14 18:08:41,299 root         INFO     New Best Identified: 	 Old Loss: inf  vs New loss:	1.3757574558258057 
2021-12-14 18:08:52,038 root         INFO     Epoch 2
2021-12-14 18:08:55,211 root         INFO     Epoch: 2 	 iteration: 0 	 Training Loss Current:1.5624 Average:(1.5624)
2021-12-14 18:09:40,827 root         INFO     Epoch: 2 	 iteration: 50 	 Training Loss Current:1.3507 Average:(1.3628)
2021-12-14 18:10:13,295 root         INFO     Average Loss for epoch 2 is 1.3025913135462603
2021-12-14 18:10:15,004 root         INFO     Validation Loss Current:1.0787 Average:(1.0787)
2021-12-14 18:10:15,074 root         INFO     Current Epoch loss is better : True 	 Old Loss: 1.3757574558258057  vs New loss:1.0786961317062378
2021-12-14 18:10:15,074 root         INFO     New Best Identified: 	 Old Loss: 1.3757574558258057  vs New loss:	1.0786961317062378 
2021-12-14 18:10:24,460 root         INFO     Epoch 3
2021-12-14 18:10:27,570 root         INFO     Epoch: 3 	 iteration: 0 	 Training Loss Current:1.2684 Average:(1.2684)
2021-12-14 18:11:13,149 root         INFO     Epoch: 3 	 iteration: 50 	 Training Loss Current:1.0964 Average:(1.1272)
2021-12-14 18:11:45,955 root         INFO     Average Loss for epoch 3 is 1.0888374951112512
2021-12-14 18:11:47,655 root         INFO     Validation Loss Current:1.0436 Average:(1.0436)
2021-12-14 18:11:47,717 root         INFO     Current Epoch loss is better : True 	 Old Loss: 1.0786961317062378  vs New loss:1.0436084270477295
2021-12-14 18:11:47,717 root         INFO     New Best Identified: 	 Old Loss: 1.0786961317062378  vs New loss:	1.0436084270477295 
2021-12-14 18:11:57,460 root         INFO     Epoch 4
2021-12-14 18:12:00,730 root         INFO     Epoch: 4 	 iteration: 0 	 Training Loss Current:0.9737 Average:(0.9737)
2021-12-14 18:12:46,730 root         INFO     Epoch: 4 	 iteration: 50 	 Training Loss Current:1.0047 Average:(0.9796)
2021-12-14 18:13:19,231 root         INFO     Average Loss for epoch 4 is 0.9783024866917635
2021-12-14 18:13:20,940 root         INFO     Validation Loss Current:0.8667 Average:(0.8667)
2021-12-14 18:13:21,007 root         INFO     Current Epoch loss is better : True 	 Old Loss: 1.0436084270477295  vs New loss:0.8666757941246033
2021-12-14 18:13:21,007 root         INFO     New Best Identified: 	 Old Loss: 1.0436084270477295  vs New loss:	0.8666757941246033 
2021-12-14 18:13:31,105 root         INFO     Epoch 5
2021-12-14 18:13:34,283 root         INFO     Epoch: 5 	 iteration: 0 	 Training Loss Current:0.8837 Average:(0.8837)
2021-12-14 18:14:19,952 root         INFO     Epoch: 5 	 iteration: 50 	 Training Loss Current:0.9218 Average:(0.9002)
2021-12-14 18:14:52,468 root         INFO     Average Loss for epoch 5 is 0.9054855055012002
2021-12-14 18:14:54,185 root         INFO     Validation Loss Current:0.8213 Average:(0.8213)
2021-12-14 18:14:54,258 root         INFO     Current Epoch loss is better : True 	 Old Loss: 0.8666757941246033  vs New loss:0.8213285803794861
2021-12-14 18:14:54,258 root         INFO     Saving Checkpoint for SegNet at epoch 5
2021-12-14 18:15:14,389 root         INFO     checkpoint saved at SegNet.pth_5.tar
2021-12-14 18:15:14,390 root         INFO     New Best Identified: 	 Old Loss: 0.8666757941246033  vs New loss:	0.8213285803794861 
2021-12-14 18:15:23,932 root         INFO     Epoch 6
2021-12-14 18:15:27,094 root         INFO     Epoch: 6 	 iteration: 0 	 Training Loss Current:0.7487 Average:(0.7487)
2021-12-14 18:16:12,614 root         INFO     Epoch: 6 	 iteration: 50 	 Training Loss Current:0.7843 Average:(0.8591)
2021-12-14 18:16:45,131 root         INFO     Average Loss for epoch 6 is 0.8414455119745876
2021-12-14 18:16:46,852 root         INFO     Validation Loss Current:0.7527 Average:(0.7527)
2021-12-14 18:16:46,923 root         INFO     Current Epoch loss is better : True 	 Old Loss: 0.8213285803794861  vs New loss:0.7526537179946899
2021-12-14 18:16:46,924 root         INFO     New Best Identified: 	 Old Loss: 0.8213285803794861  vs New loss:	0.7526537179946899 
2021-12-14 18:16:57,051 root         INFO     Epoch 7
2021-12-14 18:17:00,207 root         INFO     Epoch: 7 	 iteration: 0 	 Training Loss Current:0.7295 Average:(0.7295)
2021-12-14 18:17:45,830 root         INFO     Epoch: 7 	 iteration: 50 	 Training Loss Current:0.8245 Average:(0.8024)
2021-12-14 18:18:18,360 root         INFO     Average Loss for epoch 7 is 0.7926748420731822
2021-12-14 18:18:20,070 root         INFO     Validation Loss Current:0.7896 Average:(0.7896)
2021-12-14 18:18:20,127 root         INFO     Current Epoch loss is better : False 	 Old Loss: 0.7526537179946899  vs New loss:0.7896409034729004
2021-12-14 18:18:20,127 root         INFO     Loss didnot improve
2021-12-14 18:18:20,127 root         INFO     Epoch 8
2021-12-14 18:18:23,246 root         INFO     Epoch: 8 	 iteration: 0 	 Training Loss Current:0.7461 Average:(0.7461)
2021-12-14 18:19:08,998 root         INFO     Epoch: 8 	 iteration: 50 	 Training Loss Current:0.8290 Average:(0.7560)
2021-12-14 18:19:41,497 root         INFO     Average Loss for epoch 8 is 0.7487380801772522
2021-12-14 18:19:43,223 root         INFO     Validation Loss Current:0.7031 Average:(0.7031)
2021-12-14 18:19:43,285 root         INFO     Current Epoch loss is better : True 	 Old Loss: 0.7526537179946899  vs New loss:0.7030699253082275
2021-12-14 18:19:43,285 root         INFO     New Best Identified: 	 Old Loss: 0.7526537179946899  vs New loss:	0.7030699253082275 
2021-12-14 18:19:51,666 root         INFO     Epoch 9
2021-12-14 18:19:54,832 root         INFO     Epoch: 9 	 iteration: 0 	 Training Loss Current:0.6744 Average:(0.6744)
2021-12-14 18:20:40,536 root         INFO     Epoch: 9 	 iteration: 50 	 Training Loss Current:0.6694 Average:(0.7179)
2021-12-14 18:21:13,092 root         INFO     Average Loss for epoch 9 is 0.7170118242931641
2021-12-14 18:21:14,814 root         INFO     Validation Loss Current:0.6997 Average:(0.6997)
2021-12-14 18:21:14,873 root         INFO     Current Epoch loss is better : True 	 Old Loss: 0.7030699253082275  vs New loss:0.6996760368347168
2021-12-14 18:21:14,873 root         INFO     New Best Identified: 	 Old Loss: 0.7030699253082275  vs New loss:	0.6996760368347168 
2021-12-14 18:21:24,968 root         INFO     Epoch 10
2021-12-14 18:21:28,074 root         INFO     Epoch: 10 	 iteration: 0 	 Training Loss Current:0.6839 Average:(0.6839)
2021-12-14 18:22:13,785 root         INFO     Epoch: 10 	 iteration: 50 	 Training Loss Current:0.6658 Average:(0.6676)
2021-12-14 18:22:46,344 root         INFO     Average Loss for epoch 10 is 0.6817301046264275
2021-12-14 18:22:48,055 root         INFO     Validation Loss Current:0.6704 Average:(0.6704)
2021-12-14 18:22:48,117 root         INFO     Current Epoch loss is better : True 	 Old Loss: 0.6996760368347168  vs New loss:0.6704198122024536
2021-12-14 18:22:48,118 root         INFO     Saving Checkpoint for SegNet at epoch 10
2021-12-14 18:23:08,417 root         INFO     checkpoint saved at SegNet.pth_10.tar
2021-12-14 18:23:08,418 root         INFO     New Best Identified: 	 Old Loss: 0.6996760368347168  vs New loss:	0.6704198122024536 
2021-12-14 18:23:18,532 root         INFO     Epoch 11
2021-12-14 18:23:21,669 root         INFO     Epoch: 11 	 iteration: 0 	 Training Loss Current:0.6476 Average:(0.6476)
2021-12-14 18:24:07,256 root         INFO     Epoch: 11 	 iteration: 50 	 Training Loss Current:0.5610 Average:(0.6504)
2021-12-14 18:24:39,800 root         INFO     Average Loss for epoch 11 is 0.6488458539979258
2021-12-14 18:24:41,494 root         INFO     Validation Loss Current:0.6578 Average:(0.6578)
2021-12-14 18:24:41,553 root         INFO     Current Epoch loss is better : True 	 Old Loss: 0.6704198122024536  vs New loss:0.6578313708305359
2021-12-14 18:24:41,553 root         INFO     New Best Identified: 	 Old Loss: 0.6704198122024536  vs New loss:	0.6578313708305359 
2021-12-14 18:24:50,706 root         INFO     Epoch 12
2021-12-14 18:24:53,852 root         INFO     Epoch: 12 	 iteration: 0 	 Training Loss Current:0.6282 Average:(0.6282)
2021-12-14 18:25:39,593 root         INFO     Epoch: 12 	 iteration: 50 	 Training Loss Current:0.5207 Average:(0.6145)
2021-12-14 18:26:12,149 root         INFO     Average Loss for epoch 12 is 0.6178884583522676
2021-12-14 18:26:13,850 root         INFO     Validation Loss Current:0.5807 Average:(0.5807)
2021-12-14 18:26:13,915 root         INFO     Current Epoch loss is better : True 	 Old Loss: 0.6578313708305359  vs New loss:0.580702543258667
2021-12-14 18:26:13,915 root         INFO     New Best Identified: 	 Old Loss: 0.6578313708305359  vs New loss:	0.580702543258667 
2021-12-14 18:26:23,861 root         INFO     Epoch 13
2021-12-14 18:26:26,954 root         INFO     Epoch: 13 	 iteration: 0 	 Training Loss Current:0.5861 Average:(0.5861)
2021-12-14 18:27:12,807 root         INFO     Epoch: 13 	 iteration: 50 	 Training Loss Current:0.5462 Average:(0.5981)
2021-12-14 18:27:45,358 root         INFO     Average Loss for epoch 13 is 0.5985169694128228
2021-12-14 18:27:47,057 root         INFO     Validation Loss Current:0.5679 Average:(0.5679)
2021-12-14 18:27:47,115 root         INFO     Current Epoch loss is better : True 	 Old Loss: 0.580702543258667  vs New loss:0.5678548812866211
2021-12-14 18:27:47,116 root         INFO     New Best Identified: 	 Old Loss: 0.580702543258667  vs New loss:	0.5678548812866211 
2021-12-14 18:27:56,449 root         INFO     Epoch 14
2021-12-14 18:27:59,624 root         INFO     Epoch: 14 	 iteration: 0 	 Training Loss Current:0.5672 Average:(0.5672)
2021-12-14 18:28:45,477 root         INFO     Epoch: 14 	 iteration: 50 	 Training Loss Current:0.5245 Average:(0.5672)
2021-12-14 18:29:18,031 root         INFO     Average Loss for epoch 14 is 0.5764774515237177
2021-12-14 18:29:19,730 root         INFO     Validation Loss Current:0.6020 Average:(0.6020)
2021-12-14 18:29:19,788 root         INFO     Current Epoch loss is better : False 	 Old Loss: 0.5678548812866211  vs New loss:0.6019678711891174
2021-12-14 18:29:19,788 root         INFO     Loss didnot improve
2021-12-14 18:29:19,788 root         INFO     Epoch 15
2021-12-14 18:29:22,907 root         INFO     Epoch: 15 	 iteration: 0 	 Training Loss Current:0.5796 Average:(0.5796)
2021-12-14 18:30:08,679 root         INFO     Epoch: 15 	 iteration: 50 	 Training Loss Current:0.4685 Average:(0.5673)
2021-12-14 18:30:41,241 root         INFO     Average Loss for epoch 15 is 0.5618181860756118
2021-12-14 18:30:42,941 root         INFO     Validation Loss Current:0.5382 Average:(0.5382)
2021-12-14 18:30:43,000 root         INFO     Current Epoch loss is better : True 	 Old Loss: 0.5678548812866211  vs New loss:0.5382323861122131
2021-12-14 18:30:43,000 root         INFO     Saving Checkpoint for SegNet at epoch 15
2021-12-14 18:31:04,344 root         INFO     checkpoint saved at SegNet.pth_15.tar
2021-12-14 18:31:04,345 root         INFO     New Best Identified: 	 Old Loss: 0.5678548812866211  vs New loss:	0.5382323861122131 
2021-12-14 18:31:15,819 root         INFO     Epoch 16
2021-12-14 18:31:19,039 root         INFO     Epoch: 16 	 iteration: 0 	 Training Loss Current:0.6274 Average:(0.6274)
2021-12-14 18:32:05,905 root         INFO     Epoch: 16 	 iteration: 50 	 Training Loss Current:0.5112 Average:(0.5469)
2021-12-14 18:32:38,563 root         INFO     Average Loss for epoch 16 is 0.537416109520008
2021-12-14 18:32:40,269 root         INFO     Validation Loss Current:0.5491 Average:(0.5491)
2021-12-14 18:32:40,335 root         INFO     Current Epoch loss is better : False 	 Old Loss: 0.5382323861122131  vs New loss:0.5490787029266357
2021-12-14 18:32:40,336 root         INFO     Loss didnot improve
2021-12-14 18:32:40,336 root         INFO     Epoch 17
2021-12-14 18:32:43,631 root         INFO     Epoch: 17 	 iteration: 0 	 Training Loss Current:0.4667 Average:(0.4667)
2021-12-14 18:33:30,118 root         INFO     Epoch: 17 	 iteration: 50 	 Training Loss Current:0.4175 Average:(0.5213)
2021-12-14 18:34:03,143 root         INFO     Average Loss for epoch 17 is 0.5221864926368428
2021-12-14 18:34:04,835 root         INFO     Validation Loss Current:0.5476 Average:(0.5476)
2021-12-14 18:34:04,894 root         INFO     Current Epoch loss is better : False 	 Old Loss: 0.5382323861122131  vs New loss:0.5475953221321106
2021-12-14 18:34:04,894 root         INFO     Loss didnot improve
2021-12-14 18:34:04,894 root         INFO     Epoch 18
2021-12-14 18:34:08,005 root         INFO     Epoch: 18 	 iteration: 0 	 Training Loss Current:0.5685 Average:(0.5685)
2021-12-14 18:34:53,901 root         INFO     Epoch: 18 	 iteration: 50 	 Training Loss Current:0.5304 Average:(0.5010)
2021-12-14 18:35:27,129 root         INFO     Average Loss for epoch 18 is 0.512099538653316
2021-12-14 18:35:28,884 root         INFO     Validation Loss Current:0.5246 Average:(0.5246)
2021-12-14 18:35:28,944 root         INFO     Current Epoch loss is better : True 	 Old Loss: 0.5382323861122131  vs New loss:0.5246117115020752
2021-12-14 18:35:28,944 root         INFO     New Best Identified: 	 Old Loss: 0.5382323861122131  vs New loss:	0.5246117115020752 
2021-12-14 18:35:38,736 root         INFO     Epoch 19
2021-12-14 18:35:41,947 root         INFO     Epoch: 19 	 iteration: 0 	 Training Loss Current:0.4784 Average:(0.4784)
2021-12-14 18:36:27,927 root         INFO     Epoch: 19 	 iteration: 50 	 Training Loss Current:0.5542 Average:(0.5089)
2021-12-14 18:37:00,497 root         INFO     Average Loss for epoch 19 is 0.49714043083726844
2021-12-14 18:37:02,190 root         INFO     Validation Loss Current:0.4809 Average:(0.4809)
2021-12-14 18:37:02,248 root         INFO     Current Epoch loss is better : True 	 Old Loss: 0.5246117115020752  vs New loss:0.4808846414089203
2021-12-14 18:37:02,248 root         INFO     New Best Identified: 	 Old Loss: 0.5246117115020752  vs New loss:	0.4808846414089203 
2021-12-14 18:37:11,486 root         INFO     Epoch 20
2021-12-14 18:37:14,725 root         INFO     Epoch: 20 	 iteration: 0 	 Training Loss Current:0.4902 Average:(0.4902)
2021-12-14 18:38:00,460 root         INFO     Epoch: 20 	 iteration: 50 	 Training Loss Current:0.4671 Average:(0.4829)
2021-12-14 18:38:33,014 root         INFO     Average Loss for epoch 20 is 0.4851986243676727
2021-12-14 18:38:34,722 root         INFO     Validation Loss Current:0.5167 Average:(0.5167)
2021-12-14 18:38:34,784 root         INFO     Current Epoch loss is better : False 	 Old Loss: 0.4808846414089203  vs New loss:0.5166651010513306
2021-12-14 18:38:34,784 root         INFO     Saving Checkpoint for SegNet at epoch 20
2021-12-14 18:38:55,960 root         INFO     checkpoint saved at SegNet.pth_20.tar
2021-12-14 18:38:55,961 root         INFO     Loss didnot improve
2021-12-14 18:39:13,139 root         INFO     checkpoint saved at SegNet.pth.tar
2021-12-14 18:39:13,586 root         INFO     Plotting Charts
2021-12-14 18:39:13,586 root         INFO     Train Losses:[1.8731278902515553, 1.3025913135462603, 1.0888374951112512, 0.9783024866917635, 0.9054855055012002, 0.8414455119745876, 0.7926748420731822, 0.7487380801772522, 0.7170118242931641, 0.6817301046264275, 0.6488458539979258, 0.6178884583522676, 0.5985169694128228, 0.5764774515237177, 0.5618181860756118, 0.537416109520008, 0.5221864926368428, 0.512099538653316, 0.49714043083726844, 0.4851986243676727]
2021-12-14 18:39:13,587 root         INFO     Validation Losses:[1.3757574558258057, 1.0786961317062378, 1.0436084270477295, 0.8666757941246033, 0.8213285803794861, 0.7526537179946899, 0.7896409034729004, 0.7030699253082275, 0.6996760368347168, 0.6704198122024536, 0.6578313708305359, 0.580702543258667, 0.5678548812866211, 0.6019678711891174, 0.5382323861122131, 0.5490787029266357, 0.5475953221321106, 0.5246117115020752, 0.4808846414089203, 0.5166651010513306]
2021-12-14 18:39:13,744 root         INFO     loss curve saved at ./Plots_loss_curves.jpg.
2021-12-14 18:39:13,744 root         INFO     Training Finished for SegNet
