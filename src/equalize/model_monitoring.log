2021-12-14 19:25:35,117 root         INFO     Namespace(gpu=True, train=True, batch_size=8, num_workers=5, num_epochs=20, num_output_classes=32, learning_rate=0.01, sgd_momentum=0.5, plot_output_path='./Plots_', model_path=None, epoch_save_checkpoint=5, split_percentage=0.01, patience=50, transforms='equalize', pred_model='./checkpoint_model.pth')
2021-12-14 19:25:35,117 root         INFO     color transforms added - equalize,
2021-12-14 19:25:35,149 root         INFO     Settings for Cuda
2021-12-14 19:25:35,149 root         INFO     Training Segnet
2021-12-14 19:25:35,149 root         INFO     Generating DataLoader
2021-12-14 19:25:35,151 root         INFO     Training 1st Model
2021-12-14 19:25:38,048 root         INFO     SegNet(
  (torch_vgg16): VGG(
    (features): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): ReLU(inplace=True)
      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (3): ReLU(inplace=True)
      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (6): ReLU(inplace=True)
      (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (8): ReLU(inplace=True)
      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (11): ReLU(inplace=True)
      (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (13): ReLU(inplace=True)
      (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (15): ReLU(inplace=True)
      (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (18): ReLU(inplace=True)
      (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (20): ReLU(inplace=True)
      (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (22): ReLU(inplace=True)
      (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (25): ReLU(inplace=True)
      (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (27): ReLU(inplace=True)
      (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (29): ReLU(inplace=True)
      (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))
    (classifier): Sequential(
      (0): Linear(in_features=25088, out_features=4096, bias=True)
      (1): ReLU(inplace=True)
      (2): Dropout(p=0.5, inplace=False)
      (3): Linear(in_features=4096, out_features=4096, bias=True)
      (4): ReLU(inplace=True)
      (5): Dropout(p=0.5, inplace=False)
      (6): Linear(in_features=4096, out_features=1000, bias=True)
    )
  )
  (encoder_stage_1_conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (encoder_stage_1_conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (encoder_stage_2_conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (encoder_stage_2_conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (encoder_stage_3_conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (encoder_stage_3_conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (encoder_stage_3_conv3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (encoder_stage_4_conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (encoder_stage_4_conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (encoder_stage_4_conv3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (encoder_stage_5_conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (encoder_stage_5_conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (encoder_stage_5_conv3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (encoder_stage_1_batch_normalization1): BatchNorm2d(64, eps=1e-05, momentum=0.5, affine=True, track_running_stats=True)
  (encoder_stage_2_batch_normalization1): BatchNorm2d(128, eps=1e-05, momentum=0.5, affine=True, track_running_stats=True)
  (encoder_stage_3_batch_normalization1): BatchNorm2d(256, eps=1e-05, momentum=0.5, affine=True, track_running_stats=True)
  (encoder_stage_4_batch_normalization1): BatchNorm2d(512, eps=1e-05, momentum=0.5, affine=True, track_running_stats=True)
  (encoder_stage_5_batch_normalization1): BatchNorm2d(512, eps=1e-05, momentum=0.5, affine=True, track_running_stats=True)
  (encoder_stage_1_batch_normalization2): BatchNorm2d(64, eps=1e-05, momentum=0.5, affine=True, track_running_stats=True)
  (encoder_stage_2_batch_normalization2): BatchNorm2d(128, eps=1e-05, momentum=0.5, affine=True, track_running_stats=True)
  (encoder_stage_3_batch_normalization2): BatchNorm2d(256, eps=1e-05, momentum=0.5, affine=True, track_running_stats=True)
  (encoder_stage_4_batch_normalization2): BatchNorm2d(512, eps=1e-05, momentum=0.5, affine=True, track_running_stats=True)
  (encoder_stage_5_batch_normalization2): BatchNorm2d(512, eps=1e-05, momentum=0.5, affine=True, track_running_stats=True)
  (encoder_stage_3_batch_normalization3): BatchNorm2d(256, eps=1e-05, momentum=0.5, affine=True, track_running_stats=True)
  (encoder_stage_4_batch_normalization3): BatchNorm2d(512, eps=1e-05, momentum=0.5, affine=True, track_running_stats=True)
  (encoder_stage_5_batch_normalization3): BatchNorm2d(512, eps=1e-05, momentum=0.5, affine=True, track_running_stats=True)
  (relu): ReLU()
  (encoder_max_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (decoder_unpool): MaxUnpool2d(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))
  (decoder_stage_1_conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (decoder_stage_1_conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (decoder_stage_1_conv3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (decoder_stage_2_conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (decoder_stage_2_conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (decoder_stage_2_conv3): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (decoder_stage_3_conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (decoder_stage_3_conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (decoder_stage_3_conv3): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (decoder_stage_4_conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (decoder_stage_4_conv2): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (decoder_stage_5_conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (decoder_stage_5_conv2): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (decoder_stage_1_batch_normalization1): BatchNorm2d(512, eps=1e-05, momentum=0.5, affine=True, track_running_stats=True)
  (decoder_stage_2_batch_normalization1): BatchNorm2d(512, eps=1e-05, momentum=0.5, affine=True, track_running_stats=True)
  (decoder_stage_3_batch_normalization1): BatchNorm2d(256, eps=1e-05, momentum=0.5, affine=True, track_running_stats=True)
  (decoder_stage_4_batch_normalization1): BatchNorm2d(128, eps=1e-05, momentum=0.5, affine=True, track_running_stats=True)
  (decoder_stage_5_batch_normalization1): BatchNorm2d(64, eps=1e-05, momentum=0.5, affine=True, track_running_stats=True)
  (decoder_stage_1_batch_normalization2): BatchNorm2d(512, eps=1e-05, momentum=0.5, affine=True, track_running_stats=True)
  (decoder_stage_2_batch_normalization2): BatchNorm2d(512, eps=1e-05, momentum=0.5, affine=True, track_running_stats=True)
  (decoder_stage_3_batch_normalization2): BatchNorm2d(256, eps=1e-05, momentum=0.5, affine=True, track_running_stats=True)
  (decoder_stage_4_batch_normalization2): BatchNorm2d(64, eps=1e-05, momentum=0.5, affine=True, track_running_stats=True)
  (decoder_stage_1_batch_normalization3): BatchNorm2d(512, eps=1e-05, momentum=0.5, affine=True, track_running_stats=True)
  (decoder_stage_2_batch_normalization3): BatchNorm2d(256, eps=1e-05, momentum=0.5, affine=True, track_running_stats=True)
  (decoder_stage_3_batch_normalization3): BatchNorm2d(128, eps=1e-05, momentum=0.5, affine=True, track_running_stats=True)
)
2021-12-14 19:25:38,049 root         INFO     Epoch 1
2021-12-14 19:25:41,604 root         INFO     Epoch: 1 	 iteration: 0 	 Training Loss Current:3.5156 Average:(3.5156)
2021-12-14 19:26:27,208 root         INFO     Epoch: 1 	 iteration: 50 	 Training Loss Current:2.0375 Average:(2.3204)
2021-12-14 19:26:59,621 root         INFO     Average Loss for epoch 1 is 2.1185171095713415
2021-12-14 19:27:01,302 root         INFO     Validation Loss Current:1.5378 Average:(1.5378)
2021-12-14 19:27:01,364 root         INFO     Current Epoch loss is better : True 	 Old Loss: inf  vs New loss:1.5377649068832397
2021-12-14 19:27:01,364 root         INFO     New Best Identified: 	 Old Loss: inf  vs New loss:	1.5377649068832397 
2021-12-14 19:27:10,608 root         INFO     Epoch 2
2021-12-14 19:27:13,735 root         INFO     Epoch: 2 	 iteration: 0 	 Training Loss Current:1.7759 Average:(1.7759)
2021-12-14 19:27:59,360 root         INFO     Epoch: 2 	 iteration: 50 	 Training Loss Current:1.3783 Average:(1.4846)
2021-12-14 19:28:31,841 root         INFO     Average Loss for epoch 2 is 1.4131730784600338
2021-12-14 19:28:33,536 root         INFO     Validation Loss Current:1.1935 Average:(1.1935)
2021-12-14 19:28:33,603 root         INFO     Current Epoch loss is better : True 	 Old Loss: 1.5377649068832397  vs New loss:1.1935420036315918
2021-12-14 19:28:33,603 root         INFO     New Best Identified: 	 Old Loss: 1.5377649068832397  vs New loss:	1.1935420036315918 
2021-12-14 19:28:44,418 root         INFO     Epoch 3
2021-12-14 19:28:47,528 root         INFO     Epoch: 3 	 iteration: 0 	 Training Loss Current:1.3921 Average:(1.3921)
2021-12-14 19:29:33,165 root         INFO     Epoch: 3 	 iteration: 50 	 Training Loss Current:1.2790 Average:(1.2588)
2021-12-14 19:30:05,685 root         INFO     Average Loss for epoch 3 is 1.2247430439984763
2021-12-14 19:30:07,418 root         INFO     Validation Loss Current:1.0835 Average:(1.0835)
2021-12-14 19:30:07,478 root         INFO     Current Epoch loss is better : True 	 Old Loss: 1.1935420036315918  vs New loss:1.0835275650024414
2021-12-14 19:30:07,478 root         INFO     New Best Identified: 	 Old Loss: 1.1935420036315918  vs New loss:	1.0835275650024414 
2021-12-14 19:30:17,472 root         INFO     Epoch 4
2021-12-14 19:30:20,613 root         INFO     Epoch: 4 	 iteration: 0 	 Training Loss Current:1.0438 Average:(1.0438)
2021-12-14 19:31:06,274 root         INFO     Epoch: 4 	 iteration: 50 	 Training Loss Current:1.1219 Average:(1.1349)
2021-12-14 19:31:38,767 root         INFO     Average Loss for epoch 4 is 1.1275864293321065
2021-12-14 19:31:40,464 root         INFO     Validation Loss Current:1.0199 Average:(1.0199)
2021-12-14 19:31:40,523 root         INFO     Current Epoch loss is better : True 	 Old Loss: 1.0835275650024414  vs New loss:1.0198835134506226
2021-12-14 19:31:40,523 root         INFO     New Best Identified: 	 Old Loss: 1.0835275650024414  vs New loss:	1.0198835134506226 
2021-12-14 19:31:49,121 root         INFO     Epoch 5
2021-12-14 19:31:52,204 root         INFO     Epoch: 5 	 iteration: 0 	 Training Loss Current:0.9196 Average:(0.9196)
2021-12-14 19:32:37,904 root         INFO     Epoch: 5 	 iteration: 50 	 Training Loss Current:1.0432 Average:(1.0017)
2021-12-14 19:33:10,411 root         INFO     Average Loss for epoch 5 is 0.9922647830045189
2021-12-14 19:33:12,114 root         INFO     Validation Loss Current:0.9768 Average:(0.9768)
2021-12-14 19:33:12,171 root         INFO     Current Epoch loss is better : True 	 Old Loss: 1.0198835134506226  vs New loss:0.9768267869949341
2021-12-14 19:33:12,172 root         INFO     Saving Checkpoint for SegNet at epoch 5
2021-12-14 19:33:31,789 root         INFO     checkpoint saved at SegNet.pth_5.tar
2021-12-14 19:33:31,790 root         INFO     New Best Identified: 	 Old Loss: 1.0198835134506226  vs New loss:	0.9768267869949341 
2021-12-14 19:33:42,228 root         INFO     Epoch 6
2021-12-14 19:33:45,335 root         INFO     Epoch: 6 	 iteration: 0 	 Training Loss Current:0.8246 Average:(0.8246)
2021-12-14 19:34:30,855 root         INFO     Epoch: 6 	 iteration: 50 	 Training Loss Current:0.8025 Average:(0.9064)
2021-12-14 19:35:03,311 root         INFO     Average Loss for epoch 6 is 0.8806189107963606
2021-12-14 19:35:05,023 root         INFO     Validation Loss Current:0.7927 Average:(0.7927)
2021-12-14 19:35:05,082 root         INFO     Current Epoch loss is better : True 	 Old Loss: 0.9768267869949341  vs New loss:0.7926630973815918
2021-12-14 19:35:05,083 root         INFO     New Best Identified: 	 Old Loss: 0.9768267869949341  vs New loss:	0.7926630973815918 
2021-12-14 19:35:14,791 root         INFO     Epoch 7
2021-12-14 19:35:17,904 root         INFO     Epoch: 7 	 iteration: 0 	 Training Loss Current:0.7495 Average:(0.7495)
2021-12-14 19:36:03,598 root         INFO     Epoch: 7 	 iteration: 50 	 Training Loss Current:0.8566 Average:(0.8153)
2021-12-14 19:36:36,072 root         INFO     Average Loss for epoch 7 is 0.8032796037643718
2021-12-14 19:36:37,783 root         INFO     Validation Loss Current:0.7666 Average:(0.7666)
2021-12-14 19:36:37,844 root         INFO     Current Epoch loss is better : True 	 Old Loss: 0.7926630973815918  vs New loss:0.7665889859199524
2021-12-14 19:36:37,844 root         INFO     New Best Identified: 	 Old Loss: 0.7926630973815918  vs New loss:	0.7665889859199524 
2021-12-14 19:36:47,895 root         INFO     Epoch 8
2021-12-14 19:36:51,037 root         INFO     Epoch: 8 	 iteration: 0 	 Training Loss Current:0.7297 Average:(0.7297)
2021-12-14 19:37:36,667 root         INFO     Epoch: 8 	 iteration: 50 	 Training Loss Current:0.8587 Average:(0.7604)
2021-12-14 19:38:09,198 root         INFO     Average Loss for epoch 8 is 0.7537290785429457
2021-12-14 19:38:10,901 root         INFO     Validation Loss Current:0.7126 Average:(0.7126)
2021-12-14 19:38:10,961 root         INFO     Current Epoch loss is better : True 	 Old Loss: 0.7665889859199524  vs New loss:0.712622344493866
2021-12-14 19:38:10,962 root         INFO     New Best Identified: 	 Old Loss: 0.7665889859199524  vs New loss:	0.712622344493866 
2021-12-14 19:38:20,859 root         INFO     Epoch 9
2021-12-14 19:38:24,063 root         INFO     Epoch: 9 	 iteration: 0 	 Training Loss Current:0.6770 Average:(0.6770)
2021-12-14 19:39:09,734 root         INFO     Epoch: 9 	 iteration: 50 	 Training Loss Current:0.6545 Average:(0.7210)
2021-12-14 19:39:42,268 root         INFO     Average Loss for epoch 9 is 0.7180701817146983
2021-12-14 19:39:43,961 root         INFO     Validation Loss Current:0.6833 Average:(0.6833)
2021-12-14 19:39:44,026 root         INFO     Current Epoch loss is better : True 	 Old Loss: 0.712622344493866  vs New loss:0.6833264231681824
2021-12-14 19:39:44,026 root         INFO     New Best Identified: 	 Old Loss: 0.712622344493866  vs New loss:	0.6833264231681824 
2021-12-14 19:39:53,997 root         INFO     Epoch 10
2021-12-14 19:39:57,119 root         INFO     Epoch: 10 	 iteration: 0 	 Training Loss Current:0.6832 Average:(0.6832)
2021-12-14 19:40:42,816 root         INFO     Epoch: 10 	 iteration: 50 	 Training Loss Current:0.6566 Average:(0.6646)
2021-12-14 19:41:15,296 root         INFO     Average Loss for epoch 10 is 0.6788533269843725
2021-12-14 19:41:16,988 root         INFO     Validation Loss Current:0.6651 Average:(0.6651)
2021-12-14 19:41:17,047 root         INFO     Current Epoch loss is better : True 	 Old Loss: 0.6833264231681824  vs New loss:0.6650726795196533
2021-12-14 19:41:17,047 root         INFO     Saving Checkpoint for SegNet at epoch 10
2021-12-14 19:41:38,975 root         INFO     checkpoint saved at SegNet.pth_10.tar
2021-12-14 19:41:38,976 root         INFO     New Best Identified: 	 Old Loss: 0.6833264231681824  vs New loss:	0.6650726795196533 
2021-12-14 19:41:48,622 root         INFO     Epoch 11
2021-12-14 19:41:51,800 root         INFO     Epoch: 11 	 iteration: 0 	 Training Loss Current:0.6812 Average:(0.6812)
2021-12-14 19:42:37,316 root         INFO     Epoch: 11 	 iteration: 50 	 Training Loss Current:0.5839 Average:(0.6441)
2021-12-14 19:43:09,812 root         INFO     Average Loss for epoch 11 is 0.6454570135396908
2021-12-14 19:43:11,552 root         INFO     Validation Loss Current:0.6393 Average:(0.6393)
2021-12-14 19:43:11,629 root         INFO     Current Epoch loss is better : True 	 Old Loss: 0.6650726795196533  vs New loss:0.6392512917518616
2021-12-14 19:43:11,629 root         INFO     New Best Identified: 	 Old Loss: 0.6650726795196533  vs New loss:	0.6392512917518616 
2021-12-14 19:43:20,635 root         INFO     Epoch 12
2021-12-14 19:43:23,763 root         INFO     Epoch: 12 	 iteration: 0 	 Training Loss Current:0.6128 Average:(0.6128)
2021-12-14 19:44:09,567 root         INFO     Epoch: 12 	 iteration: 50 	 Training Loss Current:0.5534 Average:(0.6169)
2021-12-14 19:44:42,145 root         INFO     Average Loss for epoch 12 is 0.622317889753611
2021-12-14 19:44:43,847 root         INFO     Validation Loss Current:0.6042 Average:(0.6042)
2021-12-14 19:44:43,904 root         INFO     Current Epoch loss is better : True 	 Old Loss: 0.6392512917518616  vs New loss:0.6042471528053284
2021-12-14 19:44:43,904 root         INFO     New Best Identified: 	 Old Loss: 0.6392512917518616  vs New loss:	0.6042471528053284 
2021-12-14 19:44:53,204 root         INFO     Epoch 13
2021-12-14 19:44:56,351 root         INFO     Epoch: 13 	 iteration: 0 	 Training Loss Current:0.5804 Average:(0.5804)
2021-12-14 19:45:42,110 root         INFO     Epoch: 13 	 iteration: 50 	 Training Loss Current:0.5425 Average:(0.6050)
2021-12-14 19:46:14,679 root         INFO     Average Loss for epoch 13 is 0.6073127548701481
2021-12-14 19:46:16,400 root         INFO     Validation Loss Current:0.5742 Average:(0.5742)
2021-12-14 19:46:16,461 root         INFO     Current Epoch loss is better : True 	 Old Loss: 0.6042471528053284  vs New loss:0.5741883516311646
2021-12-14 19:46:16,461 root         INFO     New Best Identified: 	 Old Loss: 0.6042471528053284  vs New loss:	0.5741883516311646 
2021-12-14 19:46:26,705 root         INFO     Epoch 14
2021-12-14 19:46:29,850 root         INFO     Epoch: 14 	 iteration: 0 	 Training Loss Current:0.5687 Average:(0.5687)
2021-12-14 19:47:15,561 root         INFO     Epoch: 14 	 iteration: 50 	 Training Loss Current:0.5635 Average:(0.5743)
2021-12-14 19:47:48,100 root         INFO     Average Loss for epoch 14 is 0.5829977927359106
2021-12-14 19:47:49,816 root         INFO     Validation Loss Current:0.5771 Average:(0.5771)
2021-12-14 19:47:49,872 root         INFO     Current Epoch loss is better : False 	 Old Loss: 0.5741883516311646  vs New loss:0.5771213173866272
2021-12-14 19:47:49,872 root         INFO     Loss didnot improve
2021-12-14 19:47:49,872 root         INFO     Epoch 15
2021-12-14 19:47:53,031 root         INFO     Epoch: 15 	 iteration: 0 	 Training Loss Current:0.5774 Average:(0.5774)
2021-12-14 19:48:38,876 root         INFO     Epoch: 15 	 iteration: 50 	 Training Loss Current:0.5010 Average:(0.5826)
2021-12-14 19:49:11,483 root         INFO     Average Loss for epoch 15 is 0.5748925708899923
2021-12-14 19:49:13,189 root         INFO     Validation Loss Current:0.5430 Average:(0.5430)
2021-12-14 19:49:13,244 root         INFO     Current Epoch loss is better : True 	 Old Loss: 0.5741883516311646  vs New loss:0.5429526567459106
2021-12-14 19:49:13,245 root         INFO     Saving Checkpoint for SegNet at epoch 15
2021-12-14 19:49:33,613 root         INFO     checkpoint saved at SegNet.pth_15.tar
2021-12-14 19:49:33,614 root         INFO     New Best Identified: 	 Old Loss: 0.5741883516311646  vs New loss:	0.5429526567459106 
2021-12-14 19:49:43,620 root         INFO     Epoch 16
2021-12-14 19:49:46,738 root         INFO     Epoch: 16 	 iteration: 0 	 Training Loss Current:0.6428 Average:(0.6428)
2021-12-14 19:50:32,311 root         INFO     Epoch: 16 	 iteration: 50 	 Training Loss Current:0.5381 Average:(0.5621)
2021-12-14 19:51:04,826 root         INFO     Average Loss for epoch 16 is 0.5522692974774913
2021-12-14 19:51:06,548 root         INFO     Validation Loss Current:0.5510 Average:(0.5510)
2021-12-14 19:51:06,608 root         INFO     Current Epoch loss is better : False 	 Old Loss: 0.5429526567459106  vs New loss:0.5510237812995911
2021-12-14 19:51:06,609 root         INFO     Loss didnot improve
2021-12-14 19:51:06,609 root         INFO     Epoch 17
2021-12-14 19:51:09,698 root         INFO     Epoch: 17 	 iteration: 0 	 Training Loss Current:0.4714 Average:(0.4714)
2021-12-14 19:51:55,527 root         INFO     Epoch: 17 	 iteration: 50 	 Training Loss Current:0.4443 Average:(0.5350)
2021-12-14 19:52:28,107 root         INFO     Average Loss for epoch 17 is 0.53672596520237
2021-12-14 19:52:29,827 root         INFO     Validation Loss Current:0.5432 Average:(0.5432)
2021-12-14 19:52:29,895 root         INFO     Current Epoch loss is better : False 	 Old Loss: 0.5429526567459106  vs New loss:0.543182909488678
2021-12-14 19:52:29,895 root         INFO     Loss didnot improve
2021-12-14 19:52:29,895 root         INFO     Epoch 18
2021-12-14 19:52:33,038 root         INFO     Epoch: 18 	 iteration: 0 	 Training Loss Current:0.5861 Average:(0.5861)
2021-12-14 19:53:18,915 root         INFO     Epoch: 18 	 iteration: 50 	 Training Loss Current:0.5529 Average:(0.5180)
2021-12-14 19:53:51,514 root         INFO     Average Loss for epoch 18 is 0.5263463293097548
2021-12-14 19:53:53,230 root         INFO     Validation Loss Current:0.5293 Average:(0.5293)
2021-12-14 19:53:53,290 root         INFO     Current Epoch loss is better : True 	 Old Loss: 0.5429526567459106  vs New loss:0.5292549133300781
2021-12-14 19:53:53,291 root         INFO     New Best Identified: 	 Old Loss: 0.5429526567459106  vs New loss:	0.5292549133300781 
2021-12-14 19:54:03,052 root         INFO     Epoch 19
2021-12-14 19:54:06,235 root         INFO     Epoch: 19 	 iteration: 0 	 Training Loss Current:0.4799 Average:(0.4799)
2021-12-14 19:54:52,024 root         INFO     Epoch: 19 	 iteration: 50 	 Training Loss Current:0.5945 Average:(0.5281)
2021-12-14 19:55:24,599 root         INFO     Average Loss for epoch 19 is 0.5160486287617202
2021-12-14 19:55:26,306 root         INFO     Validation Loss Current:0.5026 Average:(0.5026)
2021-12-14 19:55:26,369 root         INFO     Current Epoch loss is better : True 	 Old Loss: 0.5292549133300781  vs New loss:0.5026015639305115
2021-12-14 19:55:26,369 root         INFO     New Best Identified: 	 Old Loss: 0.5292549133300781  vs New loss:	0.5026015639305115 
2021-12-14 19:55:36,430 root         INFO     Epoch 20
2021-12-14 19:55:39,626 root         INFO     Epoch: 20 	 iteration: 0 	 Training Loss Current:0.5385 Average:(0.5385)
2021-12-14 19:56:25,350 root         INFO     Epoch: 20 	 iteration: 50 	 Training Loss Current:0.4674 Average:(0.4973)
2021-12-14 19:56:57,882 root         INFO     Average Loss for epoch 20 is 0.49933395485369547
2021-12-14 19:56:59,602 root         INFO     Validation Loss Current:0.5044 Average:(0.5044)
2021-12-14 19:56:59,670 root         INFO     Current Epoch loss is better : False 	 Old Loss: 0.5026015639305115  vs New loss:0.5043597221374512
2021-12-14 19:56:59,670 root         INFO     Saving Checkpoint for SegNet at epoch 20
2021-12-14 19:57:23,883 root         INFO     checkpoint saved at SegNet.pth_20.tar
2021-12-14 19:57:23,884 root         INFO     Loss didnot improve
2021-12-14 19:57:45,958 root         INFO     checkpoint saved at SegNet.pth.tar
2021-12-14 19:57:46,446 root         INFO     Plotting Charts
2021-12-14 19:57:46,446 root         INFO     Train Losses:[2.1185171095713415, 1.4131730784600338, 1.2247430439984763, 1.1275864293321065, 0.9922647830045189, 0.8806189107963606, 0.8032796037643718, 0.7537290785429457, 0.7180701817146983, 0.6788533269843725, 0.6454570135396908, 0.622317889753611, 0.6073127548701481, 0.5829977927359106, 0.5748925708899923, 0.5522692974774913, 0.53672596520237, 0.5263463293097548, 0.5160486287617202, 0.49933395485369547]
2021-12-14 19:57:46,447 root         INFO     Validation Losses:[1.5377649068832397, 1.1935420036315918, 1.0835275650024414, 1.0198835134506226, 0.9768267869949341, 0.7926630973815918, 0.7665889859199524, 0.712622344493866, 0.6833264231681824, 0.6650726795196533, 0.6392512917518616, 0.6042471528053284, 0.5741883516311646, 0.5771213173866272, 0.5429526567459106, 0.5510237812995911, 0.543182909488678, 0.5292549133300781, 0.5026015639305115, 0.5043597221374512]
2021-12-14 19:57:46,605 root         INFO     loss curve saved at ./Plots_loss_curves.jpg.
2021-12-14 19:57:46,606 root         INFO     Training Finished for SegNet
